import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pypetb import RnR
import statsmodels.api as sm
from statsmodels.formula.api import ols
import scikit_posthocs as sp
from scipy.stats import chi2_contingency, chi2, mannwhitneyu, fisher_exact, linregress
from statsmodels.stats.proportion import proportions_ztest, proportion_confint
import scipy.stats as stats
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import pingouin as pg
from statsmodels.stats.power import TTestIndPower
from statistics import stdev
from typing import Tuple, Union, Optional
from scipy.stats import wilcoxon, rankdata

def gagerr(
    df,
    dict_key=None,
    show_plots=True,
    return_results=True
):
    """
    Perform Gage R&R analysis using an RnR.RnRNumeric model.

    Parameters
    ----------
    df : pandas.DataFrame
        The raw measurement data containing assessor, part, and response columns.
    dict_key : dict, optional
        A dictionary mapping required column roles. Default:
        {'1': 'Assessor', '2': 'Part', '3': 'Response'}
    show_plots : bool, optional
        Whether to display plots generated by the analysis (default: True)
    return_results : bool, optional
        Whether to return computed DataFrames (default: True)

    Returns
    -------
    results : dict
        Dictionary of DataFrames:
        {
            'anova': ANOVA table,
            'variance': Gage R&R variance table,
            'sd_table': Standard deviation table
        }
        Returned only if return_results=True
    """

    # --- Default key mapping ---
    if dict_key is None:
        dict_key = {
            '1': 'Assessor',
            '2': 'Part',
            '3': 'Response'
        }

    # --- 1. Initialize and Solve the Model ---
    print("### 1. Initializing and Solving Gage R&R Model ###")
    RnRModel = RnR.RnRNumeric(mydf_Raw=df, mydict_key=dict_key)
    RnRModel.RnRSolve()

    # --- 2. ANOVA Table ---
    print("\n### 2. ANOVA Table ###")
    df_anova = RnRModel.RnRAnova()
    print(df_anova)

    # --- 3. Gage R&R Variance Table ---
    print("\n### 3. Gage R&R Variance Table ###")
    df_var_table = RnRModel.RnR_varTable()
    print(df_var_table)

    # --- 4. Gage Evaluation Summary Plot ---
    print("\n### 4. Gage Evaluation Summary Plot ###")
    RnRModel.RnR_Report()
    if show_plots:
        plt.show()

    # --- 5. Gage Evaluation: SD Table ---
    print("\n### 5. Standard Deviation Table ###")
    df_sd_table = RnRModel.RnR_SDTable()
    print(df_sd_table)

    # --- 6. Control Charts ---
    print("\n### 6. Control Charts (X-bar and Range) ###")
    RnRModel.RnR_RunChart()
    if show_plots:
        plt.show()

    # --- Optional return of all results ---
    if return_results:
        return {
            'anova': df_anova,
            'variance': df_var_table,
            'sd_table': df_sd_table
        }


def two_proportion(successes, nobs, group_labels=None, alpha=0.05, verbose=True):
    """
    Perform both two-proportion Z-test and Fisher's Exact Test.
    """

    # --- Prepare input ---
    successes = np.asarray(successes)
    nobs = np.asarray(nobs)
    if group_labels is None:
        group_labels = ['Group 1', 'Group 2']

    # --- Descriptive statistics ---
    p = successes / nobs
    df_desc = pd.DataFrame({
        'Group': group_labels,
        'Sample Size': nobs,
        'Events': successes,
        'Proportion (%)': p * 100
    })

    # --- Z-test ---
    z_stat, p_z = proportions_ztest(successes, nobs)
    diff = p[0] - p[1]  # Group 1 minus Group 2
    se_diff = np.sqrt(p[0]*(1 - p[0])/nobs[0] + p[1]*(1 - p[1])/nobs[1])
    z_crit = stats.norm.ppf(1 - alpha/2)
    ci_diff = (diff - z_crit * se_diff, diff + z_crit * se_diff)

    # --- Fisher’s Exact Test ---
    table = np.array([
        [successes[0], nobs[0] - successes[0]],
        [successes[1], nobs[1] - successes[1]]
    ])
    odds_ratio, p_fisher = fisher_exact(table)

    # --- Confidence intervals for each proportion ---
    ci_proportion = {
        group_labels[i]: proportion_confint(successes[i], nobs[i], alpha=alpha, method='normal')
        for i in range(2)
    }

    # --- Print results ---
    if verbose:
        print("=== Two-Proportion Comparison ===")
        print(df_desc.to_string(index=False))
        print(f"\nH₀: p({group_labels[0]}) - p({group_labels[1]}) = 0")
        print(f"H₁: p({group_labels[0]}) - p({group_labels[1]}) ≠ 0")

        print("\n--- Z-Test ---")
        print(f"Z-statistic: {z_stat:.4f}")
        print(f"P-value (Z-test): {p_z:.4g}")
        print(f"Difference (p1 - p2) = {diff:.4%}  "
              f"→ Source of difference: {group_labels[0]} – {group_labels[1]}")
        print(f"{(1 - alpha)*100:.0f}% CI for diff ({group_labels[0]} - {group_labels[1]}): "
              f"({ci_diff[0]:.4f}, {ci_diff[1]:.4f})")

        print("\n--- Fisher's Exact Test ---")
        print(f"Odds ratio: {odds_ratio:.4f}")
        print(f"P-value (Fisher): {p_fisher:.4g}")

        print("\n--- Individual Group Confidence Intervals ---")
        for g in group_labels:
            print(f"{g}: {(1 - alpha)*100:.0f}% CI = {ci_proportion[g]}")

    # --- Return structured results ---
    return {
        'desc': df_desc,
        'z_stat': z_stat,
        'z_p': p_z,
        'fisher_oddsratio': odds_ratio,
        'fisher_p': p_fisher,
        'ci_proportion': ci_proportion,
        'ci_diff': ci_diff,
        'diff': diff,
        'comparison': f"{group_labels[0]} - {group_labels[1]}"
    }


def chisq(observed, row_labels=None, col_labels=None, alpha=0.05, verbose=True, top_contrib=5):
    """
    Perform both Pearson Chi-Square Test and Likelihood Ratio (G-test) 
    on a contingency table, highlighting source of association.

    Parameters
    ----------
    observed : array-like (2D)
        Observed frequency table (counts).
    row_labels : list of str, optional
        Labels for rows.
    col_labels : list of str, optional
        Labels for columns.
    alpha : float, optional
        Significance level (default: 0.05).
    verbose : bool, optional
        Print detailed outputs (default: True).
    top_contrib : int, optional
        Number of top contributing cells to highlight.

    Returns
    -------
    results : dict
        Structured results including tables, stats, and significance.
    """

    observed = np.array(observed, dtype=float)
    n_rows, n_cols = observed.shape

    if row_labels is None:
        row_labels = [f"Row {i+1}" for i in range(n_rows)]
    if col_labels is None:
        col_labels = [f"Col {j+1}" for j in range(n_cols)]

    # --- Chi-Square test ---
    chi2_stat, p_chi2, dof, expected = chi2_contingency(observed)

    # --- Pearson contributions ---
    pearson_contrib = (observed - expected)**2 / expected

    # --- Likelihood Ratio (G-test) ---
    mask = observed > 0
    lr_contrib = np.zeros_like(observed)
    lr_contrib[mask] = 2 * observed[mask] * np.log(observed[mask] / expected[mask])
    lr_stat = lr_contrib.sum()
    p_lr = 1 - chi2.cdf(lr_stat, dof)

    # --- DataFrames ---
    observed_df = pd.DataFrame(observed, index=row_labels, columns=col_labels)
    expected_df = pd.DataFrame(expected, index=row_labels, columns=col_labels)
    pearson_df = pd.DataFrame(pearson_contrib, index=row_labels, columns=col_labels)
    lr_df = pd.DataFrame(lr_contrib, index=row_labels, columns=col_labels)

    sig_chi2 = p_chi2 < alpha
    sig_lr = p_lr < alpha

    # --- Highlight top contributors ---
    def top_cells(df, n=top_contrib):
        flat = df.unstack()
        sorted_flat = flat.sort_values(ascending=False)
        return sorted_flat.head(n)

    top_pearson = top_cells(pearson_df)
    top_lr = top_cells(lr_df)

    # --- Print results ---
    if verbose:
        print("=== Chi-Square & Likelihood Ratio (G-test) ===\n")
        print("--- Observed Table ---")
        print(observed_df)
        print("\n--- Expected Frequencies ---")
        print(expected_df.round(3))
        print("\n--- Pearson Chi-Square Contributions ---")
        print(pearson_df.round(3))
        print("\nTop contributing cells (Pearson):")
        print(top_pearson)
        print("\n--- Likelihood Ratio (G-test) Contributions ---")
        print(lr_df.round(3))
        print("\nTop contributing cells (G-test):")
        print(top_lr)
        print("\n--- Summary ---")
        print(f"Chi-Square Statistic: {chi2_stat:.4f}")
        print(f"Likelihood Ratio (G-test) Statistic: {lr_stat:.4f}")
        print(f"Degrees of Freedom: {dof}")
        print(f"P-value (Chi-square): {p_chi2:.4g}")
        print(f"P-value (Likelihood ratio): {p_lr:.4g}")
        print(f"Significance Level (α): {alpha}")

        if sig_chi2:
            print("✅ Pearson Chi-Square: Reject H₀ (association exists)")
        else:
            print("❌ Pearson Chi-Square: Fail to reject H₀")

        if sig_lr:
            print("✅ Likelihood Ratio (G-test): Reject H₀ (association exists)")
        else:
            print("❌ Likelihood Ratio (G-test): Fail to reject H₀")

    return {
        'observed_df': observed_df,
        'expected_df': expected_df,
        'pearson_contrib_df': pearson_df,
        'lr_contrib_df': lr_df,
        'top_pearson': top_pearson,
        'top_lr': top_lr,
        'chi2_stat': chi2_stat,
        'lr_stat': lr_stat,
        'dof': dof,
        'p_chi2': p_chi2,
        'p_lr': p_lr,
        'significant_chi2': sig_chi2,
        'significant_lr': sig_lr
    }


def two_ttest(group1, group2, label1="Group 1", label2="Group 2", equal_var=False, alpha=0.05):
    """
    Perform a two-sample t-test (Welch or equal-variance) with descriptive statistics and 95% CI.
    Prints results and returns a structured dictionary.

    Parameters
    ----------
    group1, group2 : array-like
        Numeric data for the two groups.
    label1, label2 : str
        Names of the groups.
    equal_var : bool
        If True, use pooled variance t-test; if False, use Welch's t-test.
    alpha : float
        Significance level for confidence interval (default=0.05).

    Returns
    -------
    results : dict
        Dictionary containing descriptive stats, mean difference, CI, t-statistic, and p-value.
    """

    # --- Descriptive statistics ---
    def describe(data, label):
        n = len(data)
        mean = np.mean(data)
        std = np.std(data, ddof=1)
        sem = std / np.sqrt(n)
        return pd.Series([n, mean, std, sem], index=['n', 'Mean', 'Std', 'SE Mean'], name=label)

    desc1 = describe(group1, label1)
    desc2 = describe(group2, label2)

    # --- Mean difference and standard error ---
    mean_diff = desc1['Mean'] - desc2['Mean']
    se_diff = np.sqrt(desc1['SE Mean']**2 + desc2['SE Mean']**2)

    # --- Degrees of freedom ---
    if equal_var:
        # Pooled standard deviation
        sp2 = ((desc1['n']-1)*desc1['Std']**2 + (desc2['n']-1)*desc2['Std']**2) / (desc1['n'] + desc2['n'] - 2)
        se_diff = np.sqrt(sp2*(1/desc1['n'] + 1/desc2['n']))
        df = desc1['n'] + desc2['n'] - 2
    else:
        # Welch-Satterthwaite approximation
        df = (desc1['SE Mean']**2 + desc2['SE Mean']**2)**2 / \
             ((desc1['SE Mean']**4)/(desc1['n']-1) + (desc2['SE Mean']**4)/(desc2['n']-1))

    # --- t critical value for CI ---
    t_val = stats.t.ppf(1 - alpha/2, df=df)
    ci_lower = mean_diff - t_val*se_diff
    ci_upper = mean_diff + t_val*se_diff

    # --- t-test ---
    t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=equal_var)

    # --- Prepare results ---
    results = {
        "Descriptive Stats": pd.DataFrame([desc1, desc2]).round(5),
        "Mean Difference": round(mean_diff, 5),
        "Difference Direction": f"{label1} - {label2}",
        "95% CI": (round(ci_lower, 5), round(ci_upper, 5)),
        "t-statistic": round(t_stat, 2),
        "p-value": round(p_value, 3),
        "Degrees of Freedom": round(df, 2),
        "Equal Variance Assumed": equal_var
    }

    # --- Print descriptive statistics ---
    print("=== Descriptive Stats ===")
    print(results["Descriptive Stats"])

    # --- Test summary ---
    print("\n=== Two-Sample t-Test Summary ===")
    print(f"Difference calculated as: {label1} - {label2}")
    print(f"Mean Difference: {results['Mean Difference']}")
    print(f"95% CI: {results['95% CI']}")
    print(f"t-statistic: {results['t-statistic']}")
    print(f"p-value: {results['p-value']}")
    print(f"Degrees of Freedom: {results['Degrees of Freedom']}")
    print(f"Equal Variance Assumed: {results['Equal Variance Assumed']}")

    return results




def mannwhitney(df, group_col, value_col, group1, group2, n_boot=10000, alpha=0.05, random_state=42):
    """
    Perform Mann–Whitney U test between two groups with descriptive stats and bootstrap CI for median difference.

    Parameters
    ----------
    df : pandas.DataFrame
        Input data.
    group_col : str
        Column name containing group labels (categorical, e.g., 'Shift').
    value_col : str
        Column name containing numeric values (e.g., 'Defects per Airbag').
    group1, group2 : str
        Labels of the two groups to compare.
    n_boot : int
        Number of bootstrap resamples for median difference CI.
    alpha : float
        Significance level for hypothesis test.
    random_state : int
        Random seed for reproducibility.

    Returns
    -------
    summary : pandas.DataFrame
        Descriptive stats for each group + median difference row.
    test_results : dict
        Mann–Whitney U statistic and p-value with interpretation.
    """

    # --- Split data ---
    data1 = df.loc[df[group_col] == group1, value_col].dropna().values
    data2 = df.loc[df[group_col] == group2, value_col].dropna().values

    # --- Mann–Whitney U test ---
    stat, p_value = mannwhitneyu(data1, data2, alternative="two-sided")
    interpretation = ("Reject null hypothesis → distributions differ significantly."
                      if p_value < alpha else "Fail to reject null → no significant difference.")

    test_results = {
        "Mann–Whitney U": round(stat, 4),
        "p-value": round(p_value, 4),
        "Interpretation": interpretation
    }

    # --- Descriptive statistics ---
    desc = pd.DataFrame({
        "Group": [group1, group2],
        "N": [len(data1), len(data2)],
        "Mean": [np.mean(data1), np.mean(data2)],
        "Std Dev": [np.std(data1, ddof=1), np.std(data2, ddof=1)],
        "SE Mean": [np.std(data1, ddof=1)/np.sqrt(len(data1)),
                    np.std(data2, ddof=1)/np.sqrt(len(data2))],
        "Median": [np.median(data1), np.median(data2)],
        "IQR": [np.percentile(data1, 75) - np.percentile(data1, 25),
                np.percentile(data2, 75) - np.percentile(data2, 25)]
    })

    # --- Observed median difference ---
    median_diff = np.median(data1) - np.median(data2)

    # --- Bootstrap CI for median difference ---
    rng = np.random.default_rng(random_state)
    diffs = np.empty(n_boot)
    for i in range(n_boot):
        sample1 = rng.choice(data1, size=len(data1), replace=True)
        sample2 = rng.choice(data2, size=len(data2), replace=True)
        diffs[i] = np.median(sample1) - np.median(sample2)
    ci_lower, ci_upper = np.percentile(diffs, [100*alpha/2, 100*(1-alpha/2)])

    # --- Add difference summary row ---
    diff_row = pd.DataFrame({
        "Group": ["Median Difference ({} - {})".format(group1, group2)],
        "N": [np.nan],
        "Mean": [np.nan],
        "Std Dev": [np.nan],
        "SE Mean": [np.nan],
        "Median": [median_diff],
        "IQR": [f"[{ci_lower:.4f}, {ci_upper:.4f}]"]
    })

    # --- Combine into final table ---
    summary = pd.concat([desc, diff_row], ignore_index=True)

    print("Descriptive Statistics:")
    print(summary)

    print("\nMann–Whitney Test Results:")
    for k, v in test_results.items():
        print(f"{k}: {v}")

    return summary.round(4), test_results



def sample_size_2ttest(mean1, mean2, std1, std2, alpha=0.05, power=0.8, alternative='two-sided'):
    """
    Calculate required sample size per group for a two-sample t-test.
    
    Parameters:
    - mean1, mean2: group means
    - std1, std2: standard deviations of the groups
    - alpha: significance level
    - power: desired statistical power
    - alternative: 'two-sided', 'larger', or 'smaller'
    
    Returns:
    - required sample size per group
    """
    # Validate alternative
    if alternative not in ['two-sided', 'larger', 'smaller']:
        raise ValueError("alternative must be 'two-sided', 'larger', or 'smaller'")
    
    # Pooled standard deviation
    pooled_std = np.sqrt((std1**2 + std2**2) / 2)
    
    # Effect size
    effect_size = abs(mean1 - mean2) / pooled_std
    
    # Power analysis
    analysis = TTestIndPower()
    n_per_group = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative=alternative)

    print(f"\nRequired sample size per group: {n_per_group:.0f}")
    return n_per_group



def paired_ttest(device_a, device_b, alpha=0.05, plot=True, label_a="Device A", label_b="Device B"):
    """
    Perform paired t-test analysis with descriptive statistics and boxplot.
    
    Parameters:
    - device_a, device_b: pandas Series or arrays of paired measurements
    - alpha: significance level for confidence intervals and hypothesis test
    - plot: whether to show a boxplot of differences
    - label_a, label_b: optional custom names for display
    
    Returns:
    - desc_stats: DataFrame of descriptive statistics
    - ci: tuple of (lower, upper) 95% confidence interval for mean difference
    - t_stat, p_value: results of the paired t-test
    """

    # --- Compute paired differences (A - B) ---
    device_a = device_a.dropna()
    device_b = device_b.dropna()
    diff = device_a - device_b
    n = len(diff)
    
    # --- Descriptive statistics ---
    mean_a, std_a = device_a.mean(), device_a.std(ddof=1)
    mean_b, std_b = device_b.mean(), device_b.std(ddof=1)
    mean_diff, std_diff = diff.mean(), diff.std(ddof=1)
    se_diff = std_diff / np.sqrt(n)
    
    # --- Confidence interval ---
    t_crit = stats.t.ppf(1 - alpha/2, df=n-1)
    ci_lower = mean_diff - t_crit * se_diff
    ci_upper = mean_diff + t_crit * se_diff
    
    desc_stats = pd.DataFrame({
        label_a: [n, mean_a, std_a, device_a.min(), device_a.max()],
        label_b: [n, mean_b, std_b, device_b.min(), device_b.max()],
        f"Difference ({label_a} - {label_b})": [n, mean_diff, std_diff, diff.min(), diff.max()]
    }, index=["N", "Mean", "Std", "Min", "Max"])
    
    # --- Paired t-test ---
    t_stat, p_value = stats.ttest_rel(device_a, device_b)
    
    # --- Print results ---
    print("=== Descriptive Statistics ===")
    print(desc_stats.round(6))
    
    print(f"\nDifference calculated as: {label_a} - {label_b}")
    print(f"{100*(1-alpha):.0f}% CI for mean difference: ({ci_lower:.6f}, {ci_upper:.6f})")
    print(f"Mean difference = {mean_diff:.6f}")
    
    print("\n=== Paired t-test ===")
    print(f"t-statistic: {t_stat:.3f}")
    print(f"p-value: {p_value:.3f}")
    if p_value < alpha:
        print("Reject null hypothesis: mean difference is significant")
    else:
        print("Fail to reject null hypothesis: no significant difference")
    
    # --- Box plot ---
    if plot:
        plt.figure(figsize=(6,6))
        plt.boxplot(diff, labels=[f"Difference ({label_a} - {label_b})"])
        plt.title("Box Plot of Paired Differences")
        plt.ylabel(f"Difference ({label_a} - {label_b})")
        plt.grid(alpha=0.3)
        plt.show()
    
    return {'desc_stats': desc_stats,
            '(ci_lower, ci_upper)': (ci_lower, ci_upper),
            't_stat': t_stat, 
            'p_value': p_value
    }



def wilcoxon_test(data, mu=0, alternative='two-sided'):
    """
    Perform a one-sample Wilcoxon signed-rank test.
    
    Parameters
    ----------
    data : array-like
        Numerical data (list, numpy array, or pandas Series).
    mu : float, default=0
        Hypothesized median under the null hypothesis.
    alternative : {'two-sided', 'greater', 'less'}, default='two-sided'
        Defines the alternative hypothesis.
    
    Returns
    -------
    pd.DataFrame
        Summary table with sample size, median, test statistic, p-value,
        null hypothesis, and alternative hypothesis.
    """
    data = np.array(data)
    data = data[~np.isnan(data)]  # remove NaN values
    
    n = len(data)
    median_val = np.median(data)
    
    # Perform Wilcoxon test
    stat, p = wilcoxon(data - mu, alternative=alternative, method='exact')
    
    # Create summary
    summary = pd.DataFrame({
        "N": [n],
        "Median": [median_val],
        "Wilcoxon Statistic": [stat],
        "p-value": [p],
        "Null Hypothesis": [f"median = {mu}"],
        "Alternative Hypothesis": [
            f"median {'≠' if alternative=='two-sided' else ('>' if alternative=='greater' else '<')} {mu}"
        ]
    })
    
    print(summary)

    return summary





def check_var(
    df: pd.DataFrame,
    columns: list | None = None,
    alpha: float = 0.05,
    method: str = "levene",
    show_summary: bool = True
):
    """
    Check variance equality across groups and summarize statistics per group,
    including sample size, standard deviation, and confidence interval (Bonferroni-adjusted).

    Parameters
    ----------
    df : pd.DataFrame
        Each column represents a group (e.g., Lot 1, Lot 2, ...).
    columns : list of str, optional
        Columns to include in the test. Default is None (all columns used).
    alpha : float, default=0.05
        Significance level for variance test.
    method : {"levene", "bartlett", "bartlett_minitab"}, default="levene"
        Variance equality test.
    show_summary : bool, default=True
        Print summary table and test results.

    Returns
    -------
    dict
        {
            "summary": DataFrame,  # per-group stats
            "variance_test": dict  # test statistic, p-value, decision
        }
    """

    # --- Select columns ---
    if columns is not None:
        df = df[columns]

    groups = [df[col].dropna().values for col in df.columns]
    group_labels = df.columns.tolist()
    
    # --- Compute per-group summary statistics ---
    summary_list = []
    k = len(groups)
    for i, g in enumerate(groups):
        n = len(g)
        stdev = np.std(g, ddof=1)
        # Chi-squared confidence interval for standard deviation
        chi2_lower = stats.chi2.ppf(alpha/(2*k), df=n-1)
        chi2_upper = stats.chi2.ppf(1 - alpha/(2*k), df=n-1)
        ci_lower = np.sqrt((n-1)*stdev**2 / chi2_upper)
        ci_upper = np.sqrt((n-1)*stdev**2 / chi2_lower)
        summary_list.append({
            "Sample": group_labels[i],
            "N": n,
            "StDev": stdev,
            "CI Lower": ci_lower,
            "CI Upper": ci_upper
        })
    summary_df = pd.DataFrame(summary_list)

    # --- Variance equality test ---
    if method.lower() == "levene":
        stat, p = stats.levene(*groups, center="mean")
        method_used = "Levene’s Test (center=mean)"
    elif method.lower() == "bartlett":
        stat, p = stats.bartlett(*groups)
        method_used = "Bartlett’s Test (classic)"
    elif method.lower() == "bartlett_minitab":
        n_i = np.array([len(g) for g in groups])
        s_i2 = np.array([np.var(g, ddof=1) for g in groups])
        N = np.sum(n_i)
        s_p2 = np.sum((n_i-1)*s_i2)/(N-k)
        C = (N-k)*np.log(s_p2) - np.sum((n_i-1)*np.log(s_i2))
        num = 1 + (1/(3*(k-1)))*(np.sum(1/(n_i-1)) - 1/(N-k))
        stat = C / num
        p = 1 - stats.chi2.cdf(stat, k-1)
        method_used = "Bartlett’s Test (Minitab correction)"
    else:
        raise ValueError("method must be 'levene', 'bartlett', or 'bartlett_minitab'")
    
    equal_variance = p >= alpha
    conclusion = ("Equal variances assumed" if equal_variance else "Variances NOT equal")
    
    variance_test = {
        "method": method_used,
        "statistic": stat,
        "p_value": p,
        "equal_variance": equal_variance,
        "conclusion": conclusion
    }

    # --- Print summary ---
    if show_summary:
        print("\n--- Summary Statistics (per group) ---")
        print(summary_df.to_string(index=False, float_format="{:.4f}".format))
        print(f"\n--- Variance Equality Test ---")
        print(f"Method: {method_used}")
        print(f"Statistic = {stat:.4f}")
        print(f"p-value   = {p:.4f}")
        print(f"Conclusion (α={alpha}): {conclusion}")

    return {
        "summary": summary_df,
        "variance_test": variance_test
    }


def anova(
    df: pd.DataFrame,
    columns: list | None = None,
    alpha: float = 0.05,
    method: str = "fisher",
    show_summary: bool = True
):
    """
    Perform one-way ANOVA with options for Fisher (equal variances) or 
    Games-Howell (unequal variances) post-hoc tests.

    Parameters
    ----------
    df : pd.DataFrame
        Each column represents a group (e.g., Lot 1, Lot 2, ...).
    columns : list of str, optional
        Columns to include. Default = all columns.
    alpha : float, default=0.05
        Significance level.
    method : {"fisher", "games-howell"}, default="fisher"
        Choose the ANOVA method.
    show_summary : bool, default=True
        Print results.

    Returns
    -------
    dict
        {
            "anova_table": DataFrame,
            "posthoc_table": DataFrame or None,
            "F": float,
            "p_value": float,
            "method": str,
            "conclusion": str
        }
    """

    if columns is not None:
        df = df[columns]

    # Convert to long format for statsmodels / post-hoc
    df_long = df.melt(var_name="Group", value_name="Value")

    if method.lower() == "fisher":
        # Classical ANOVA (equal variance)
        model = ols("Value ~ C(Group)", data=df_long).fit()
        anova_table = sm.stats.anova_lm(model, typ=2)
        F = anova_table["F"][0]
        p = anova_table["PR(>F)"][0]

        # Tukey HSD post-hoc
        tukey = pairwise_tukeyhsd(df_long['Value'], df_long['Group'], alpha=alpha)
        posthoc = pd.DataFrame(data=tukey._results_table.data[1:], columns=tukey._results_table.data[0])

        method_name = "Fisher ANOVA + Tukey HSD"

    elif method.lower() == "games-howell":
        # Welch ANOVA (unequal variance) using pingouin
        welch_res = pg.welch_anova(dv='Value', between='Group', data=df_long)
        F = welch_res['F'].values[0]
        p = welch_res['p-unc'].values[0]
        anova_table = welch_res

        # Games-Howell post-hoc
        posthoc = pg.pairwise_gameshowell(dv='Value', between='Group', data=df_long)

        method_name = "Welch ANOVA + Games-Howell"

    else:
        raise ValueError("method must be 'fisher' or 'games-howell'")

    conclusion = ("Reject H0 (Means differ significantly, p < {:.3f})".format(alpha)
                  if p < alpha else
                  "Fail to reject H0 (No significant difference, p ≥ {:.3f})".format(alpha))

    if show_summary:
        print(f"\n--- ANOVA Results ({method_name}) ---")
        print(anova_table)
        print(f"\nF = {F:.4f}, p = {p:.4f}")
        print(f"Conclusion (α={alpha}): {conclusion}")
        print("\n--- Post-hoc pairwise comparisons ---")
        print(posthoc.round(4))

    return {
        "anova_table": anova_table,
        "posthoc_table": posthoc,
        "F": F,
        "p_value": p,
        "method": method_name,
        "conclusion": conclusion
    }


def anom(df, columns=None, alpha=0.05, show_table=True, title=None):
    """
    Perform classical ANOM (Analysis of Means) for means assuming equal variances.

    Parameters
    ----------
    df : pd.DataFrame
        Each column is a group (e.g., Lot1, Lot2, ...).
    columns : list of str, optional
        Subset of columns to include.
    alpha : float, default=0.05
        Significance level for decision limits.
    show_table : bool, default=True
        Whether to print summary table.
    title : str, optional
        Chart title.

    Returns
    -------
    dict
        {
            "grand_mean": float,
            "pooled_std": float,
            "decision_limits": (lower, upper),
            "group_means": pd.DataFrame
        }
    """
    if columns is not None:
        df = df[columns]

    k = df.shape[1]  # number of groups
    n_i = df.count()  # sample size per group
    means = df.mean()
    grand_mean = df.stack().mean()

    # --- Pooled standard deviation ---
    df_flat = df.count().sum() - k
    squared_sum = sum(((df[col] - means[col])**2).sum() for col in df.columns)
    s_pooled_sq = squared_sum / df_flat
    s_pooled = np.sqrt(s_pooled_sq)

    # --- ANOM critical value h (approx using t) ---
    # For exact Minitab match, replace with studentized-range q value
    q_alpha = stats.t.ppf(1 - alpha/2, df=df_flat) * np.sqrt(2)  # approximate
    lower = grand_mean - q_alpha * s_pooled / np.sqrt(n_i)
    upper = grand_mean + q_alpha * s_pooled / np.sqrt(n_i)

    # --- Group summary table ---
    summary = pd.DataFrame({
        "Mean": means,
        "n": n_i,
        "Lower Limit": lower,
        "Upper Limit": upper,
        "Out of Limits": (means < lower) | (means > upper)
    })

    if show_table:
        print(f"\n--- ANOM for Means (α={alpha}) ---")
        print(f"Grand Mean: {grand_mean:.4f}, Pooled Std: {s_pooled:.4f}")
        print(summary.round(4))

    # --- Plot ANOM chart ---
    plt.figure(figsize=(8, max(4, k*0.5)))
    y_pos = np.arange(k)
    plt.errorbar(means.values, y_pos, xerr=[means.values - lower.values, upper.values - means.values],
                 fmt='o', color="#2563EB", ecolor='gray', capsize=5, lw=2)
    plt.axvline(grand_mean, color='red', linestyle='--', label='Grand Mean')
    plt.yticks(y_pos, means.index)
    plt.xlabel("Mean Value")
    plt.ylabel("Groups")
    if title is None:
        title = "ANOM Chart for Means"
    plt.title(title)
    plt.grid(alpha=0.3, linestyle='--')
    plt.ylim(-0.5, k - 0.5 + 0.5)  # add 0.5 padding at top
    plt.legend()
    plt.tight_layout()
    plt.show()

    return {
        "grand_mean": grand_mean,
        "pooled_std": s_pooled,
        "decision_limits": (lower, upper),
        "group_means": summary
    }
